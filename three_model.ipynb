{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# config inline plotting of matplotlib, use svg format for better quality and smaller file size\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the Model Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetSmall(nn.Module):\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(AlexNetSmall, self).__init__() # Call parent class constructor\n",
    "\n",
    "        # Define the layers of the network\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size = 3, stride = 1, padding = 1), # 3 input channels, 32 output channels, 3x3 kernel\n",
    "            nn.ReLU(inplace = True), # ReLU activation\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2), # Max pooling with 2x2 kernel and stride 2\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding = 1),   # 32 input channels, 64 output channels, 3x3 kernel\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "\n",
    "            nn.Conv2d(64, 96, kernel_size = 3, padding = 1), \n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(96, 64, kernel_size = 3, padding = 1),  \n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(64, 32, kernel_size = 3, padding = 1), \n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        )\n",
    "\n",
    "        # Define the classifier part of the network, which is a fully connected network\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p = 0.5),  # Dropout with probability 0.5 to prevent overfitting\n",
    "            nn.Linear(32 * 4 * 4, 512),  # Fully connected layer with 512 units, 32*4*4 is the size of the output of the convolutional part\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(512, 256),  # Fully connected layer with 256 units, 512 is the size of the output of the previous layer\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(256, num_classes), # Output layer with num_classes units (10 in this case)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    # forward method defines the computation performed at every call\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # Convolutional layers\n",
    "        x = torch.flatten(x, 1)  # Flatten the output of the convolutional layers\n",
    "        x = self.classifier(x)  # Fully connected layers\n",
    "        return x\n",
    "\n",
    "    # Added weight initialization method\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():  # Iterate over all modules in the network\n",
    "            if isinstance(m, nn.Conv2d): # Check if the module is a Conv2d layer\n",
    "                # Initialize Conv2d weights with kaiming_normal initialization\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None: # Check if the layer has bias\n",
    "                    nn.init.constant_(m.bias, 0) # Initialize bias to 0\n",
    "            elif isinstance(m, nn.Linear): # Check if the module is a Linear layer\n",
    "                nn.init.normal_(m.weight, 0, 0.01) # Initialize Linear weights with normal distribution (mean 0, std 0.01)\n",
    "                nn.init.constant_(m.bias, 0)  # Initialize bias to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(BasicCNN, self).__init__()\n",
    "\n",
    "        # Define the layers of the network using Sequential API\n",
    "        self.features = nn.Sequential(\n",
    "            # First block\n",
    "            nn.Conv2d(3, 32, kernel_size = 3, padding = 1), # 3 input channels, 32 output channels, 3x3 kernel\n",
    "            nn.BatchNorm2d(32), # normalize the output of the previous layerï¼Œhelps to stabilize and speed up the training\n",
    "            nn.ReLU(inplace = True), # ReLU activation function to introduce non-linearity\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2), # Max pooling with 2x2 kernel and stride 2\n",
    "\n",
    "            # Second block\n",
    "            nn.Conv2d(32, 64, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(64), # normalize the output of the previous layer\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "\n",
    "            # Third block\n",
    "            nn.Conv2d(64, 128, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "\n",
    "            # Fourth block\n",
    "            nn.Conv2d(128, 256, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "        )\n",
    "\n",
    "        # Define the classifier part of the network, which is a fully connected network \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5), \n",
    "            nn.Linear(256 * 2 * 2, 512), # Fully connected layer with 512 units, 256*2*2 is the size of the output of the convolutional part\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes) # Output layer with num_classes units (10 in this case)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights() \n",
    "\n",
    "    # forward method defines the computation performed at every call\n",
    "    def forward(self, x):\n",
    "        x = self.features(x) # Convolutional layers\n",
    "        x = torch.flatten(x, 1) # Flatten the output of the convolutional layers\n",
    "        x = self.classifier(x) # Fully connected layers\n",
    "        return x\n",
    "\n",
    "    # Added weight initialization method\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 ResNet_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1  # expansion factor for the number of output channels, used in the shortcut connection, default is 1\n",
    "\n",
    "    # constructor of the ResidualBlock class\n",
    "    def __init__(self, in_channels, out_channels, stride = 1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)  # first convolution\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)  # batch normalization\n",
    "        self.relu = nn.ReLU(inplace = True)  # ReLU activation function\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)  # second convolution\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)  # batch normalization\n",
    "\n",
    "        # shortcut connection to add to the output of the second batch normalization\n",
    "        self.shortcut = nn.Sequential() \n",
    "        # if the number of input channels is different from the number of output channels, we need to apply a convolution to the shortcut\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1,stride=stride, bias=False),  # convolution for the shortcut\n",
    "                # batch normalization for the shortcut\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x) # shortcut connection to add to the output of the second batch normalization\n",
    "        out = self.conv1(x)  # first convolution\n",
    "        out = self.bn1(out)  # batch normalization\n",
    "        out = self.relu(out)  # ReLU activation function\n",
    "        out = self.conv2(out) \n",
    "        out = self.bn2(out)  \n",
    "        out += identity  # add the shortcut connection, this is the skip connection, the output of the second batch normalization is added to the shortcut\n",
    "        out = self.relu(out) \n",
    "        return out\n",
    "\n",
    "\n",
    "# ResNet18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64  # number of input channels\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias = False)  # first convolution\n",
    "        self.bn1 = nn.BatchNorm2d(64) \n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.layer1 = self.make_layer(64, 2, stride=1)  # first residual block\n",
    "        self.layer2 = self.make_layer(128, 2, stride=2)  # second residual block\n",
    "        self.layer3 = self.make_layer(256, 2, stride=2)  # third residual block\n",
    "        self.layer4 = self.make_layer(512, 2, stride=2)  # fourth residual block\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # average pooling\n",
    "        self.fc = nn.Linear(512 * ResidualBlock.expansion, num_classes)  # fully connected layer \n",
    "\n",
    "    # function to create a residual block\n",
    "    def make_layer(self, out_channels, num_blocks, stride):\n",
    "        # strides for the residual block\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []  # list to store the layers of the residual block\n",
    "        for stride in strides:\n",
    "            # add a residual block to the list\n",
    "            layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
    "            # update the number of input channels for the next residual block\n",
    "            self.in_channels = out_channels * ResidualBlock.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x) # first convolution\n",
    "        out = self.bn1(out) # batch normalization\n",
    "        out = self.relu(out) # ReLU activation function\n",
    "        out = self.layer1(out) # first residual block\n",
    "        out = self.layer2(out) # second residual block\n",
    "        out = self.layer3(out) # third residual block\n",
    "        out = self.layer4(out) # fourth residual block\n",
    "        out = self.avg_pool(out) # average pooling\n",
    "        out = out.view(out.size(0), -1) # flatten the output of the average pooling\n",
    "        out = self.fc(out) # fully connected layer\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define PyTorch Lightning Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    # Define the data module for CIFAR-10 dataset, which is a subclass of LightningDataModule class\n",
    "    def __init__(self, data_dir: str = './data', batch_size: int = 128, num_workers: int = 4, augmentation_strength: str = 'basic'):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir # directory to store the data\n",
    "        self.batch_size = batch_size # batch size\n",
    "        self.num_workers = num_workers # number of workers for data loaders\n",
    "        self.augmentation_strength = augmentation_strength # augmentation strength, can be 'basic' or 'strong'\n",
    "\n",
    "        # Define transforms based on augmentation strength\n",
    "        if augmentation_strength == 'basic':\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding = 4), # random crop with padding 4\n",
    "                transforms.RandomHorizontalFlip(), # random horizontal flip\n",
    "                transforms.ToTensor(), # convert the image to PyTorch tensor\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) # normalize the image with mean and standard deviation\n",
    "            ])\n",
    "        elif augmentation_strength == 'strong':\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15), # random rotation with maximum angle 15 degrees\n",
    "                transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)), # random affine transformation with maximum translation 0.1 in both directions\n",
    "                transforms.ColorJitter(brightness = 0.2, contrast = 0.2, saturation = 0.2), # random color jitter\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown augmentation strength: {augmentation_strength}\") # raise an error if the augmentation strength is unknown\n",
    "\n",
    "        # Define test transform without augmentation (only normalization)\n",
    "        self.test_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        ])\n",
    "\n",
    "    # Prepare the data by downloading the CIFAR-10 dataset\n",
    "    def prepare_data(self):\n",
    "        datasets.CIFAR10(self.data_dir, train = True, download = True) # download training data \n",
    "        datasets.CIFAR10(self.data_dir, train = False, download = True) # download test data\n",
    "\n",
    "    # Setup the data module by defining the train, validation, and test datasets\n",
    "    def setup(self, stage: Optional[str] = None): # stage can be 'fit' (training), 'validate' (validation), 'test', or None\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = datasets.CIFAR10(self.data_dir, train = True, transform = self.transform) # training dataset\n",
    "            train_size = int(0.9 * len(self.train_dataset)) # 90% training, 10% validation\n",
    "            val_size = len(self.train_dataset) - train_size \n",
    "            # split the training dataset into training and validation datasets based on the sizes \n",
    "            self.train_dataset, self.val_dataset = torch.utils.data.random_split(self.train_dataset, [train_size, val_size])\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_dataset = datasets.CIFAR10(self.data_dir, train = False, transform = self.test_transform)\n",
    "\n",
    "    # Define data loaders for trainingï¼Œshuffle the training data\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size = self.batch_size, shuffle = True, num_workers = self.num_workers)\n",
    "\n",
    "    # Define data loaders for validation\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size = self.batch_size, num_workers = self.num_workers)\n",
    "\n",
    "    # Define data loaders for testing\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size = self.batch_size, num_workers = self.num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define Base Model, Log Metrics and Extra Parameters Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base model and log the hyperparameters, training, validation, and test metrics, and extra parameters information\n",
    "\n",
    "# Base model class\n",
    "class BaseModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 model_name: str, \n",
    "                 learning_rate: float = 1e-3, \n",
    "                 optimizer: str = 'Adam', \n",
    "                 augmentation_strength: str = 'basic' \n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate  # learning rate for the optimizer\n",
    "        self.model_name = model_name  # model name, can be 'alex_net', 'basic_cnn', or 'res_net_18'\n",
    "        self.optimizer = optimizer # optimizer, can be 'Adam' or 'SGD'\n",
    "        self.augmentation_strength = augmentation_strength # augmentation strength, can be 'basic' or 'strong'\n",
    "\n",
    "        # Initialize the appropriate model\n",
    "        if model_name == 'alex_net':\n",
    "            self.model = AlexNetSmall(num_classes = 10)\n",
    "        elif model_name == 'basic_cnn':\n",
    "            self.model = BasicCNN(num_classes = 10)\n",
    "        elif model_name == 'res_net_18':\n",
    "            self.model = ResNet18(num_classes = 10)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model name: {model_name}\") # raise an error if the model name is unknown\n",
    "\n",
    "        # Save hyperparameters to the log file, accessible by all the callbacks\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Metrics for training, validation, and testing\n",
    "        self.train_acc = Accuracy(task = 'multiclass', num_classes = 10) # task is multiclass classification, num_classes is 10\n",
    "        self.val_acc = Accuracy(task = 'multiclass', num_classes = 10)\n",
    "        self.test_acc = Accuracy(task = 'multiclass', num_classes = 10)\n",
    "\n",
    "    # Forward pass of the model to compute the output\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    #define the optimizer and the learning rate scheduler\n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams.optimizer == 'Adam': # Adam optimizer\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr = self.learning_rate)\n",
    "        elif self.hparams.optimizer == 'SGD': # SGD optimizer\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr = self.learning_rate)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {self.hparams.optimizer}\")\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 200) # CosineAnnealingLR learning rate scheduler\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"val_loss\" # monitor the validation loss for the learning rate scheduler\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Training step to compute the loss and update the weightsï¼Œlog the training loss and accuracy\n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int):\n",
    "        x, y = batch # input and target labels\n",
    "        logits = self(x) # forward pass\n",
    "        loss = F.cross_entropy(logits, y) # cross-entropy loss\n",
    "\n",
    "        # Log metrics\n",
    "        acc = self.train_acc(logits.softmax(dim = -1), y) # compute accuracy using the softmax of the logits and the target labels\n",
    "        self.log('train_loss', loss, prog_bar = True) # log the training loss\n",
    "        self.log('train_acc', acc, prog_bar = True) # log the training accuracy\n",
    "\n",
    "        # Add extra parameters information\n",
    "        self.logger.experiment.add_scalar('parameters/learning_rate', self.learning_rate) # learning rate to the log file \n",
    "        self.logger.experiment.add_text('parameters/optimizer', self.optimizer) # optimizer to the log file\n",
    "        self.logger.experiment.add_text('parameters/augmentation_strength', self.hparams.augmentation_strength) \n",
    "\n",
    "        return loss\n",
    "\n",
    "    # Validation step to compute the loss and accuracy, log the validation loss and accuracy\n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int):\n",
    "        x, y = batch # input and target labels\n",
    "        logits = self(x) \n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # Log metrics \n",
    "        acc = self.val_acc(logits.softmax(dim = -1), y)\n",
    "        self.log('val_loss', loss, prog_bar = True) \n",
    "        self.log('val_acc', acc, prog_bar = True)\n",
    "\n",
    "        # Add extra parameters information\n",
    "        self.logger.experiment.add_scalar('parameters/learning_rate', self.learning_rate)\n",
    "        self.logger.experiment.add_text('parameters/optimizer', self.optimizer)\n",
    "        self.logger.experiment.add_text('parameters/augmentation_strength', self.hparams.augmentation_strength)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # Test step to compute the loss and accuracy, log the test loss and accuracy\n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int):\n",
    "        x, y = batch # input and target labels\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # Log metrics\n",
    "        acc = self.test_acc(logits.softmax(dim = -1), y)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)\n",
    "\n",
    "        # Add extra parameters information\n",
    "        self.logger.experiment.add_scalar('parameters/learning_rate', self.learning_rate)\n",
    "        self.logger.experiment.add_text('parameters/optimizer', self.optimizer)\n",
    "        self.logger.experiment.add_text('parameters/augmentation_strength', self.hparams.augmentation_strength)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model_name: str,\n",
    "        learning_rate: float = 1e-3, \n",
    "        optimizer: str = 'Adam', \n",
    "        augmentation_strength: str = 'basic',\n",
    "        max_epochs: int = 20 # maximum number of epochs\n",
    "):\n",
    "    # Create data module\n",
    "    data_module = CIFAR10DataModule(augmentation_strength = augmentation_strength)\n",
    "\n",
    "    # Create model\n",
    "    model = BaseModel(model_name = model_name, learning_rate = learning_rate, optimizer = optimizer)\n",
    "\n",
    "    version = f\"lr_{learning_rate}_opt_{optimizer}_aug_{augmentation_strength}\" # version based on hyperparameters\n",
    "\n",
    "    # Create logger for TensorBoard visualization and logging the metrics \n",
    "    logger = TensorBoardLogger(\n",
    "        'lightning_logs', # directory to store the logs\n",
    "        name = model_name, # model name\n",
    "        version = version # version based on hyperparameters\n",
    "    )\n",
    "\n",
    "    # Create checkpoint callback to save the best model based on validation loss\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor = 'val_loss', # monitor the validation loss for saving the best model\n",
    "        dirpath = f'checkpoints/{model_name}', # directory to store the checkpoints\n",
    "        filename = '{epoch:02d}-{val_loss:.2f}', # file name format for the checkpoints\n",
    "        save_top_k = 3, # save the top 3 models based on validation loss\n",
    "        mode = 'min' # minimize the validation loss\n",
    "    )\n",
    "\n",
    "    # Create early stopping callback to stop the training if the validation loss does not improve for 10 epochs\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        patience = 10,\n",
    "        mode = 'min'\n",
    "    )\n",
    "\n",
    "    # Create trainer to train the model\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs = max_epochs, # maximum number of epochs\n",
    "        accelerator = 'auto', # use GPU if available\n",
    "        devices = \"auto\", # use GPU if available\n",
    "        strategy = \"auto\", # use distributed training if available\n",
    "        logger = logger, # logger for TensorBoard visualization\n",
    "        callbacks = [checkpoint_callback, early_stopping], # list of callbacks \n",
    "        deterministic = True # set to True for reproducibility, but it may slow down the training\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.fit(model, data_module)\n",
    "\n",
    "    # Test model\n",
    "    trainer.test(model, data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Main Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define the grid of hyperparameters\n",
    "    param_grid = {\n",
    "        'models': ['basic_cnn', 'alex_net',  'res_net_18'],\n",
    "        'learning_rate': [1e-2, 1e-3, 1e-4],\n",
    "        'optimizer': ['Adam', 'SGD'],\n",
    "        'augmentation_strength': ['basic', 'strong']\n",
    "    }\n",
    "    # Train all models with different hyperparameters\n",
    "    for model_name in param_grid['models']:\n",
    "        for lr in param_grid['learning_rate']:\n",
    "            for optimizer in param_grid['optimizer']:\n",
    "                for augmentation_strength in param_grid['augmentation_strength']:\n",
    "                    print(\n",
    "                        f\"Training model: {model_name}, lr: {lr}, optimizer: {optimizer}, augmentation_strength: {augmentation_strength}\")\n",
    "                    train_model(model_name, lr, optimizer, augmentation_strength)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss](./train_loss_all.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![valLoss](./val_loss_all.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![trainacc](./train_acc_all.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![valacc](./val_acc_all.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
